# ğŸ”Š Audio MNIST Digit Recognition

## ğŸ“Œ Project Overview
This project focuses on **classifying spoken digits (0-9) from the Audio MNIST dataset** using **Deep Learning models**. The goal is to convert raw audio data into **Mel-Frequency Cepstral Coefficients (MFCCs)** and use them for training a neural network model.

## ğŸ¯ Objective
- Extract features from raw speech data using **MFCC spectrograms**.
- Train an **Artificial Neural Network (ANN)** to classify spoken digits.
- Evaluate model performance using **accuracy and confusion matrix**.

## ğŸ“‚ Dataset Description
The dataset consists of **spoken digit recordings** stored as **.wav audio files**. Each file corresponds to a speaker pronouncing a digit (0-9). The dataset is structured into folders, with each folder representing a different speaker.

### ğŸ” Data Fields:
| Feature       | Description |
|--------------|-------------|
| `digit`      | The spoken digit (0-9) |
| `speaker_id` | Unique identifier for each speaker |
| `mfcc_features` | Extracted MFCC coefficients |
| `audio_waveform` | Raw waveform representation |

## ğŸ“Š Exploratory Data Analysis (EDA)
- **Visualized waveform representations** of different spoken digits.
- **Converted raw audio to spectrograms** using **MFCC feature extraction**.
- **Analyzed class distribution** to ensure balanced datasets.

## ğŸ—ï¸ Model Building
### ğŸ”¢ Data Preprocessing
- **Extracted MFCC features** from raw audio.
- **Standardized** numerical features.
- **Split dataset** into **75% training and 25% testing**.

### ğŸ“ˆ Machine Learning Model
1. **Artificial Neural Network (ANN)**
   - **3 hidden layers** with **ReLU activation**.
   - **Softmax output layer** for multi-class classification.
   - **Adam optimizer** with sparse categorical cross-entropy loss.

## ğŸ“Š Model Evaluation
- **Metrics used**: Accuracy, Precision, Recall, Confusion Matrix.
- **Final accuracy**: **Achieved >95% accuracy on test data**.

## ğŸ› ï¸ Tech Stack
- **Python** (Librosa, NumPy, Pandas, Matplotlib, TensorFlow, Keras)
- **Deep Learning Model**: Fully Connected ANN

## ğŸ”® Key Findings & Recommendations
- **MFCC spectrograms are highly effective** for speech classification.
- **ANN performed well**, but **CNNs or RNNs could improve accuracy**.
- **Adding more speaker variations** would enhance model robustness.

## ğŸ“Œ Next Steps
- Experiment with **CNNs and LSTMs** for speech recognition.
- Fine-tune hyperparameters for better generalization.
- Deploy the model as a **real-time voice recognition system**.

---

### ğŸ“ Project Structure
```
ğŸ“‚ Audio_MNIST_Digit_Recognition
â”‚â”€â”€ ğŸ“œ Audio_MNIST_Digit_Recognition.ipynb  # Jupyter Notebook with full analysis
â”‚â”€â”€ ğŸ“Š Audio_MNIST_Digit_Recognition.pdf    # Project summary & visualizations
â”‚â”€â”€ ğŸ“œ README.md                            # Project documentation
â”‚â”€â”€ ğŸ“Š Data/                                # Dataset (if applicable)
â”‚â”€â”€ ğŸ“œ requirements.txt                     # Dependencies
```

## ğŸ“§ Contact & Contributions
- Author: **Agostino Fontana**
- Contributions are welcome! Feel free to open an issue or submit a pull request.

---
ğŸš€ *If you found this project useful, feel free to â­ the repository!* ğŸ˜Š
